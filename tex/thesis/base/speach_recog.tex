\section{アンカーの発話区間の音声認識実験}

\subsection{実験方法}
本実験では、\ref{section:yoshimura_pre_clustering}節で述べた木構造話者クラスタを作成し、話者クラスタに含まれる学習データを用いて音響モデルを学習して音声認識実験を行う。各話者クラスタに含まれる男女の発話データ数を図\ref{fig:yoshimura_kikouzou}に示す。


\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.5]{./figure/yoshimura.eps}
  \end{center}
  \caption{各話者クラスタに含まれる発話データ数 \label{fig:yoshimura_kikouzou}}
\end{figure}

ここで、学習データに用いた話者のi-vectorと認識するアンカーの発話のi-vectorのコサイン類似度を求め、求めたコサイン類似度の高い上位5人の学習データを全て含んでいるクラスタに含まれる学習音声で学習された音響モデルを用いて音声認識を行う。音響モデルの選択に用いるi-vectorと認識するアンカーの発話区間は\ref{chapter:get_anchor}節で検出したアンカーの発話区間のうち、各手法でもっともF値の高かった条件のものを用いる。\par

本実験で使用した音響モデル、言語モデル、単語辞書の仕様は\ref{section:experiment_acoustic_model}節、\ref{section:experiment_language_model}節で述べる。

\subsection{音響モデルの仕様}
\label{section:experiment_acoustic_model}
本実験で用いたDNN-HMM音響モデルの仕様を表\ref{table:acoustic_model_detail}に示す。この仕様に関しては小島らの研究\cite{kojima}で使用されたもので、状態数は3000、音響特徴の次元数は39次元(表\ref{acoustic_model_feature})、隠れ層の数は6層、各層における繰り返し学習数は5回、隠れ層のノード数は1024とした。以下に、DNNを用いた際の学習の手順を示す。

\begin{table}[H]
  \begin{center}
    \caption{音響モデルの仕様 \label{table:acoustic_model_detail}}
    \begin{tabular}{|c|c|c|} \hline
     状態数  & 使用した音素 & 混合数 \\ \hline
     3,000  & 27 & 16 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{table}[H]
  \begin{center}
    \caption{使用する音響特徴パラメータ \label{acoustic_model_feature}}
    \begin{tabular}{|c||c|} \hline
      特徴量 & 次元数\\ \hline
      MFCC & 12  \\ \hline
      POW & 1  \\ \hline
      $\Delta$MFCC & 12 \\ \hline
      $\Delta$POW & 1 \\ \hline
      $\Delta\Delta$MFCC & 12 \\ \hline
      $\Delta\Delta$POW & 1 \\ \hline
      計 & 39 \\ \hline
    \end{tabular}
  \end{center}
\end{table}



\vspace{0.2in}\noindent{\textbf{\underline{使用した音素}}}\par
本研究で使用した音素27個を表\ref{fig:used_onso}に示す。また、その音素をもとに記したカナ音素対応表を表\ref{fig:kana_onso}に示す


\begin{table}[H]
\begin{center}
\caption{使用した音素 \label{fig:used_onso}}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
母音  & 子音         & 濁音  & 半濁音 & 撥音 & 促音 & 無音 \\ \hline
a i & ch f h j k & b d &     &    &    &    \\ 
u e & m n r s sh & g z & p   & ng & q  & \# \\ 
o   & t ts w     & zh  &     &    &    &    \\ \hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[H]
  \begin{center}
    \caption{カナ音素対応表 \label{fig:kana_onso}}
    \includegraphics[scale=0.7]{./figure/kana_onso.eps}
  \end{center}
  
\end{table}

\subsection{言語モデル・単語辞書の仕様}
\label{section:experiment_language_model}
言語モデルはトライグラムモデルを構築した。以下、使用した学習テキストを説明する。

\vspace{0.2in}\noindent{\textbf{\underline{CSJ}}}\par
CSJには書き起こしテキストも提供されており、その一部の例を図\ref{fig:kakiokosi}に示す。書き起こしテキストは主に情報部と発話部に区別される。情報部では発話IDや時間情報等を、発話部では発話内容を「＆」の左側に基本形、右側に発音形という形式で記している。発話形はカタカナを用いて実際に発音された音声を忠実に表記したものである。発音の怠けや言い間違い等を書き取れる範囲で忠実に記録している。本研究では、音響モデル構築の際には主に発話部の発音形を用い、このカタカナ表記を音素列に変換し、ラベルファイルとして定義する。
\begin{figure}[H]
  \begin{center}
    \includegraphics{./figure/kakiokosi.eps}
  \end{center}
  \caption{書き起こしテキストの例 \label{fig:kakiokosi}}
\end{figure}

本研究ではこのCSJをベースに学習テキストを構成する。使用するデータは977講演分のテキストで、約14MBである。

\vspace{0.2in}\noindent{\textbf{\underline{拡張したコーパスによる学習テキスト}}}\par
この学習テキストは江頭らによる、学術講演の書き起こしと新聞記事に拡張されるテキストとして参加者名の入ったテキスト、Webから収集してきたテキスト、そして対話コーパスから作成される対話テキストを追加した未知語の減少に着目した学習テキストである。この学習テキストは会議中に参加者の名前を呼ぶことが多い、会議は対話形式であるなどの会議の特徴を考慮した学習テキストである。テキストサイズは約100MBである。以降本論文では、このテキストを拡張したコーパスによる学習テキストと呼ぶ。

\vspace{0.2in}\noindent{\textbf{\underline{拡張したコーパスによる学習テキスト}}}\par
この学習テキストは荒井らによる、会議における発話行為に着目して作成された学習テキストである。学術講演の書き起こしと新聞記事に対話表現に近い特徴を持っていると考えられるQ＆Aサイトから収集したテキストと対話コーパスを追加した学習テキストである。テキストサイズは約44MBである。以降本論文ではこのテキストを対話特化テキストと呼ぶ。


\subsection{評価方法}
本研究では評価尺度としては式\ref{calc:word_acc}で与えられる単語正解精度$Acc$(Word Accuracy)を用いる。ここで$W$は単語数、$S$(Substitution)は置換誤り、$D$(Deletion)は脱落誤り、$I$(Insertions)は挿入誤りの単語数を表わす。置換誤りとは、正解の単語が別の単語に誤認識された場合の誤りである。脱落誤りとは、単語があるべき部分に認識結果が何も出力されなかった場合の誤りである。挿入誤りは、本来単語がない部分に誤認識結果として単語が出力された場合の誤りである。

\begin{equation}
\label{calc:word_acc}
Acc=\frac{(W-S-D-I)}{W}
\end{equation}

          
評価は、正解ファイルと認識結果のファイルをDPマッチングを行なうことにより算出する。この正解ファイルは形態素解析した結果の形態素列によって作成したものである。


また、本研究ではアンカーの発話区間が既知の場合と未知の場合で音声認識精度の評価を行う。アンカーの発話区間が未知の時、アンカー以外の発話区間で認識された単語は全て挿入誤り、アンカーの発話として検出出来なかった発話区間の単語は全て削除誤りとして計算する。

\subsection{実験結果}
各手法で抽出されたi-vectorを元に、各手法における発話データの音響モデルの選択結果を図\ref{fig:baseline_clustering} $\sim$ 図\ref{fig:prob3_clustering}に示す。

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.5]{./figure/baseline_clustering.eps}
  \end{center}
  \caption{Baselineによる音響モデルの選択結果 \label{fig:baseline_clustering}}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.5]{./figure/prob1_clustering.eps}
  \end{center}
  \caption{手法1による音響モデルの選択結果 \label{fig:prob1_clustering}}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.5]{./figure/prob2_clustering.eps}
  \end{center}
  \caption{手法2による音響モデルの選択結果 \label{fig:prob2_clustering}}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.5]{./figure/prob3_clustering.eps}
  \end{center}
  \caption{手法3による音響モデルの選択結果 \label{fig:prob3_clustering}}
\end{figure}

Baselineと比較して、いずれの手法も最上位の音響モデルを選択する発話データ数が減少し、下位クラスタを選択する発話データが増えている。\\

\noindent{\textbf{\underline{アンカーの発話区間が既知の場合}}}\par
アンカーの発話区間が既知の場合の音声認識結果を表\ref{table:result_sprecog1}に示す。

\begin{table}[H]
  \begin{center}
    \caption{アンカーの発話区間が既知の場合の音声認識結果 \label{table:result_sprecog1}}
    \begin{tabular}{|c||c|c|c|c|c|} \hline
     手法  & $Acc$ & Substitution & Deletion & Insertions \\ \hline
     Baseline  & 61.6 & 463 & 307 & 1834 \\ \hline
     手法1  & 61.6 & 477 & 318 & 1813 \\ \hline
     手法2  & 61.7 & 460 & 305 & 1836 \\ \hline
     手法3  & 61.6 & 453 & 304 & 1827 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

Baselineと比較して、いずれの手法も認識精度の向上は確認できなかった。

\vspace{0.2in}\noindent{\textbf{\underline{アンカーの発話区間が未知の場合}}}  \par
アンカーの発話区間が未知の場合の音声認識結果を表\ref{table:result_sprecog2}に示す。

\begin{table}[H]
  \begin{center}
    \caption{アンカーの発話区間が未知の場合の音声認識結果 \label{table:result_sprecog2}}
    \begin{tabular}{|c||c|c|c|c|} \hline
     手法  & $Acc$ & Substitution & Deletion & Insertions \\ \hline
     Baseline & 26.7 & 957 & 2334 & 1684 \\ \hline
     手法1  & 29.9 & 994 & 2080 & 1681 \\ \hline
     手法2  & 35.4 & 1014 & 1711 & 1657 \\ \hline
     手法3  & 35.4 & 998 & 1713 & 1670 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

アンカーの発話区間が未知の場合はいずれも発話区間が既知の場合と比較して大きく音声認識精度が低下した。また、いずれの手法もBaselineと比較して音声認識精度は向上している。

\subsection{考察}
本研究で提案した手法はいずれもBaselineと比較して下位のクラスタを選択する発話データが増加した。これは、発話区間を結合したことで、i-vectorが性別の違いを判別できる程度の特徴を抽出できたためであると考えられる。\par
アンカーの発話区間が既知の場合に音声認識精度がいずれも変化がなかった理由として、背景雑音、音楽の存在が考えられる。音響モデルの学習に用いたCSJは基本的に雑音が入らない環境で収録されている。このため、本実験で作成した木構造話者クラスの音響モデルのいずれも認識できない発話が多く存在してしまい、認識精度の違いがなかったと考えられる。音声認識精度の向上のために、雑音除去、もしくは雑音、音楽に頑健な音響モデルの作成が必要であると考えらえる。\par
アンカーの発話区間が未知の場合はいずれも発話区間が既知の場合と比較して大きく音声認識精度が低下した理由として、アンカー以外の発話区間で認識された単語は全て挿入誤り、アンカーの発話として検出出来なかった発話区間の単語は全て削除誤りとして計算したためである。また、いずれの手法もBaselineと比較してアンカーの発話区間検出精度が向上していたため、削除誤りと挿入誤りが少なくなり、結果として音声認識精度が向上した。しかし、最も音声認識精度が高い場合でも35.4\%であるため、音声認識精度を向上させるためには、アンカーの発話区間検出精度を向上させる必要がある。
